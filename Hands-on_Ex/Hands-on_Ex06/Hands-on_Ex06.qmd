---
title: "Hands-on Exercise 6"
author: "Brigitta Karen Tsai"
date: "September 25, 2024"
date-modified: "last-modified"
execute: 
  eval: true
  echo: true
  message: false
  freeze: true
---

# Geographical Segmentation with Spatially Constrained Clustering Techniques

## 1 Getting Started

### 1.1 Installing and Loading R Packages

```{r}
pacman::p_load(spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

### 1.2 Importing Data into R environment

#### 1.2.1 Importing Geospatial Data

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

```{r}
shan_sf
```

```{r}
glimpse(shan_sf)
```

#### 1.2.2 Importing Aspatial Data

```{r}
ict <- read_csv ("data/aspatial/Shan-ICT.csv")
```

```{r}
summary(ict)
```

#### 1.2.3 Deriving New Variables using dplyr packages

The current unit of measurement of the values are "number of household". We cannot directly use this value as it will be bias by the underlying total number of household. In general, townships with relatively higher total number of households will also have higher number of household owning radio, TV, etc.

To overcome this problem, we will derive the penetration rate of each ICT variable

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

```{r}
summary(ict_derived)
```

## 2 Exploratory Data Analysis (EDA)

### 2.1 EDA using statistical graphics

Plot histogram to see overall distribution of the variables

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Boxplot to detect outliers

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

Distribution of the newly derived variable

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

Before the new variable is derived, the histogram was right-skewed. It is because the data was biased from the total number of household in each township. Using the newly derived variable, the number of household with radio is slightly right-skewed. It means there are 6 townships in which their penetration rate for radio is around 250.

### 2.2 EDA using choropleth map

#### 2.2.1 Joining geospatial data and aspatial data

```{r}
shan_join <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
write_rds(shan_join, "data/rds/shan_join.rds")
```

```{r}
shan_join <- read_rds("data/rds/shan_join.rds")
```

#### 2.2.2 Preparing a choropleth map

```{r}
qtm(shan_join, "RADIO_PR")
```

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_join) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_join) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

Now, plot choropleth maps showing the distribution of total number of households and radio penetration rate

```{r}
tm_shape(shan_join) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

## 3 Correlation Analysis

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggests that only one of them should be used in the cluster analysis.

## 4 Hierarchy Cluster Analysis

### 4.1 Extracting Clustering Variables

```{r}
cluster_vars <- shan_join %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

Next, change the rows by township name instead of row number

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

Remove TS.x

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

### 4.2 Data Standardisation

In cluster analysis, multiple variables are used and it's not unusual if their values range are different. To avoid biasness, we need to standardise the data.

#### 4.2.1 Min-Max Standardisatiom

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

#### 4.2.2 Z-score Standardisation

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

### 4.3 Visualising the standardised clustering variables

The code below plots the scaled RADIO_PR field

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

With standardisation, the distribution value is now standardised within a certain range (0-1 or -2 to 2)

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

### 4.4 Computing proximity matrix

The code below computes proximity matrix using euclidean method

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

```{r}
proxmat
```

### 4.5 Computing Hierarchical Clustering

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

```{r}
plot(hclust_ward, cex = 0.6)
```

### 4.6 Selecting the optimal clustering algorithm

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

From the code result above, we can see that the Ward's method provides the strongest clustering structure. Hence, Ward's method will be used

### 4.7 Determining Optimal Clusters

#### 4.7.1 Gap Statistic Method

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

```{r}
fviz_gap_stat(gap_stat)
```

From the graph, it is recommended to use 1 cluster, however, it is not logical to use 1 cluster. By examining the gap statistic graph, the **6 cluster** gives the largest gap statistics and should be the best cluster to pick.

#### 4.7.2 Average Silhouette Method

```{r}
set.seed(12345)
silhouette_stat <- fviz_nbclust(shan_ict, hcut, method = "silhouette", 
                                k.max = 10, nstart = 25)

print(silhouette_stat)
```

From the result above, the optimum number of clusters is 2.

#### 4.7.3 Elbow Method

```{r}
set.seed(12345)
elbow_stat <- fviz_nbclust(shan_ict, hcut, method = "wss", 
                           k.max = 10, nstart = 25)

print(elbow_stat)
```

From the result above, the optimum number of clusters is 2.

### 4.8 Interpreting Dendogram

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

### 4.9 Visually-driven hierarchical clustering analysis

Using **heatmaply**, we are able to build both interactive or static cluster heatmap

#### 4.9.1 Transforming data frame into a matrix

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

#### 4.9.2 Plotting interactive cluster heatmap

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )
```

### 4.10 Mapping the clusters formed

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```

## 5 Spatially Constrained Clustering: SKATER Approach

### 5.1 Converting into SpatialPolygonsDataFrame

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### 5.2 Computing Neighbour List

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

```{r}
coords <- st_coordinates(
  st_centroid(st_geometry(shan_sf)))
```

```{r}
plot(st_geometry(shan_sf), 
     border=grey(.5))
plot(shan.nb,
     coords, 
     col="blue", 
     add=TRUE)
```

### 5.3 Computing minimum spanning tree

#### 5.3.1 Calculating edge costs

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

We specify the style as **B** to make sure the cost values are not row-standardised

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

### 5.4 Computing minimum spanning tree

```{r}
shan.mst <- mstree(shan.w)
```

Check class and dimension

```{r}
class(shan.mst)
```

```{r}
dim(shan.mst)
```

Dimension is not 55 because the minimum spanning tree consists on n-1 edged to traverse all the nodes

```{r}
head(shan.mst)
```

```{r}
plot(st_geometry(shan_sf), 
                 border=gray(.5))
plot.mst(shan.mst, 
         coords, 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

### 5.5 Computing spatially constrained clusters using SKATER method

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

```{r}
str(clust6)
```

Check cluster assignment

```{r}
ccs6 <- clust6$groups
ccs6
```

We can find out the no. of observations in each cluster. We can also find this as the dimension of each vector in the lists contained in edges.

```{r}
table(ccs6)
```

For example, the first list has node with dimension 12, which is also the number of observations in the first cluster

Plot the pruned tree that shows 5 clusters on top of the township area

```{r}
plot(st_geometry(shan_sf), 
     border=gray(.5))
plot(clust6, 
     coords, 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

### 5.6 Visualising the clusters in choropleth map

Plot the newly derived clusters using SKATER method

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")
```

```{r}
hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

## 6 Spatially Constrained Clustering: ClustGeo Method

### 6.1 Ward-like hierarchical clustering: ClustGeo

To perform non-spatially constrained hierarchical clustering, we only need to provide the function as dissimilarity matrix

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

#### 6.1.1 Mapping the clusters formed

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
```

```{r}
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

### 6.2 Spatially Constrained Hierarchical Clustering

Before performing spatially constrained hierarchical clustering, a spatial distance matrix will be derived

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

Next, determine a suitable value for the mixing parameter alpha

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

According to the graph, alpha = 0.3 will be used

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
```

Derive cluster object

```{r}
groups <- as.factor(cutree(clustG, k=6))
```

Join back the group list with shan_sf polygon feature data frame

```{r}
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

```{r}
qtm(shan_sf_Gcluster, "CLUSTER")
```

## 7 Visual Interpretation of Clusters

### 7.1 Visualising individual clustering variable

\* Note: column RADIO_PR is not found in the dataframe.
